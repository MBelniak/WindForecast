# @package optim
# Example of PyTorch optimizer usage

defaults:
  - schema_optim
  - schema_optim_adam

optimizer:
  _target_: torch.optim.Adam
  params: ~

  lr: 0.001
  weight_decay: 0
  betas: [0.9, 0.999]
  amsgrad: false

scheduler:
  _target_: torch.optim.lr_scheduler.LambdaLR
