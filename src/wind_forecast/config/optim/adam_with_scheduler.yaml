# @package optim
# Example of PyTorch optimizer usage

defaults:
  - schema_optim
  - schema_optim_adam

optimizer:
  _target_: torch.optim.Adam
  params: ~

  lr: 0.001
  weight_decay: 0
  betas: [0.9, 0.999]
  amsgrad: false

scheduler:
  _target_: torch.optim.lr_scheduler.LambdaLR

final_lr: 0.00001
starting_lr: 0.0001
lambda_lr:
  _target_: wind_forecast.lr_schedulers.transformer_lr_scheduler.TransformerLRScheduler
