from bs4 import BeautifulSoup
import requests
import re
from pathlib import Path
import os

def get_synop_data(year: str, dir: str):
    dir_per_year = os.path.join(dir, year)
    Path(dir_per_year).mkdir(parents=True, exist_ok=True)

    url ="https://dane.imgw.pl/data/dane_pomiarowo_obserwacyjne/dane_meteorologiczne/terminowe/synop/" + year

    page = requests.get(url)

    soup = BeautifulSoup(page.content, 'html.parser')
    zip_rows = soup.find_all('tr')

    for row in zip_rows[:5]:
        td_link = row.find('a')
        if td_link:
            contains_zip_file = re.match(r'^([a-zA-Z0-9\s_\\.\-\(\):])+\.zip$', td_link['href'])
            if contains_zip_file:
                file = requests.get(url + td_link['href'], allow_redirects=True)
                open(os.path.join(dir_per_year, td_link['href']), 'wb').write(file.content)

        # if td_link["href"]


if __name__ == "__main__":
    dir = './synop_data'
    get_synop_data('2019', dir)